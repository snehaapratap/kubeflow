# PIPELINE DEFINITION
# Name: ml-pipeline
components:
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        ytest_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        metrics_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-load-data:
    executorLabel: exec-load-data
    outputDefinitions:
      artifacts:
        output_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-preprocess-data:
    executorLabel: exec-preprocess-data
    inputDefinitions:
      artifacts:
        input_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        output_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        output_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        output_ytest:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        output_ytrain:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        train_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        ytrain_data:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        model_output:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(test_data: Input[Dataset], ytest_data: Input[Dataset],\
          \ model: Input[Model], metrics_output: Output[Dataset]):\n    import subprocess\n\
          \    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"\
          , \"matplotlib\", \"joblib\"], check=True)\n\n    import pandas as pd\n\
          \    from sklearn.metrics import classification_report, confusion_matrix\n\
          \    import matplotlib.pyplot as plt\n    from joblib import load\n\n  \
          \  # Load test data\n    X_test = pd.read_csv(test_data.path)\n\n    y_test\
          \ = pd.read_csv(ytest_data.path)  \n\n    # Load model\n    model = load(model.path)\n\
          \n    # Predict\n    y_pred = model.predict(X_test)\n\n    # Generate metrics\n\
          \    report = classification_report(y_test, y_pred, output_dict=True)\n\
          \    cm = confusion_matrix(y_test, y_pred)\n\n    # Save metrics to a file\n\
          \    metrics_path = metrics_output.path\n    with open(metrics_path, 'w')\
          \ as f:\n        f.write(str(report))\n\n    # Plot confusion matrix\n \
          \   plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest',\
          \ cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n\
          \    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.savefig(metrics_path.replace('.txt',\
          \ '.png'))\n\n"
        image: python:3.9
    exec-load-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data(output_csv: Output[Dataset]):\n    import subprocess\n\
          \    subprocess.run([\"pip\", \"install\", \"pandas\", \"scikit-learn\"\
          ], check=True)\n\n    from sklearn.datasets import load_iris\n    import\
          \ pandas as pd\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data,\
          \ columns=iris.feature_names)\n    df['target'] = iris.target\n\n    # Save\
          \ the dataset to the output artifact path\n    df.to_csv(output_csv.path,\
          \ index=False)\n\n"
        image: python:3.9
    exec-preprocess-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_data(input_csv: Input[Dataset], output_train: Output[Dataset],\
          \ output_test: Output[Dataset], \n                    output_ytrain: Output[Dataset],\
          \ output_ytest: Output[Dataset]):\n    import subprocess\n    subprocess.run([\"\
          pip\", \"install\", \"pandas\", \"scikit-learn\"], check=True)\n\n    import\
          \ pandas as pd\n    from sklearn.preprocessing import StandardScaler\n \
          \   from sklearn.model_selection import train_test_split\n\n    # Load dataset\n\
          \    df = pd.read_csv(input_csv.path)\n\n    # Debug: Check for NaN values\n\
          \    print(\"Initial dataset shape:\", df.shape)\n    print(\"Missing values\
          \ before preprocessing:\\n\", df.isnull().sum())\n\n    # Handle missing\
          \ values\n    if df.isnull().values.any():\n        print(\"Missing values\
          \ detected. Handling them...\")\n        df = df.dropna()  # Drop rows with\
          \ any NaN values\n\n    # Validate that there are no NaNs in the target\
          \ column\n    assert not df['target'].isnull().any(), \"Target column contains\
          \ NaN values after handling missing values.\"\n\n    features = df.drop(columns=['target'])\n\
          \    target = df['target']\n\n    # Standardize features\n    scaler = StandardScaler()\n\
          \    scaled_features = scaler.fit_transform(features)\n\n    # Train-test\
          \ split\n    X_train, X_test, y_train, y_test = train_test_split(scaled_features,\
          \ target, test_size=0.2, random_state=42)\n\n    # Debug: Validate splits\n\
          \    print(\"Shapes after train-test split:\")\n    print(\"X_train:\",\
          \ X_train.shape, \"X_test:\", X_test.shape) \n    print(\"y_train:\", y_train.shape,\
          \ \"y_test:\", y_test.shape)\n    print(\"Missing values in y_train:\",\
          \ y_train.isnull().sum())\n\n    # Ensure no NaNs in the split data\n  \
          \  assert not y_train.isnull().any(), \"y_train contains NaN values.\"\n\
          \    assert not y_test.isnull().any(), \"y_test contains NaN values.\"\n\
          \n    # Create DataFrames for train and test sets\n    X_train_df = pd.DataFrame(X_train,\
          \ columns=features.columns)\n    print(\"X_train_df:\", X_train_df) \n\n\
          \    y_train_df = pd.DataFrame(y_train) \n    print(\"y_train_df: \", y_train_df)\
          \  \n\n    X_test_df = pd.DataFrame(X_test, columns=features.columns)\n\
          \    print(\"X_test_df:\", X_test_df) \n\n    y_test_df = pd.DataFrame(y_test)\
          \ \n    print(\"y_test_df: \", y_test_df) \n\n    # Save processed train\
          \ and test data\n    X_train_df.to_csv(output_train.path, index=False) \
          \ \n    X_test_df.to_csv(output_test.path, index=False)\n\n    y_train_df.to_csv(output_ytrain.path,\
          \ index=False)  \n    y_test_df.to_csv(output_ytest.path, index=False) \n\
          \n"
        image: python:3.9
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(train_data: Input[Dataset], ytrain_data: Input[Dataset],\
          \ model_output: Output[Model]):\n\n    import subprocess\n    subprocess.run([\"\
          pip\", \"install\", \"pandas\", \"scikit-learn\", \"joblib\"], check=True)\n\
          \n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n\
          \    from joblib import dump\n\n    # Load training data\n    train_df =\
          \ pd.read_csv(train_data.path)\n    print(\"Shape of train_df:\", train_df.shape)\n\
          \    print(\"train_df:\", train_df)\n    X_train = train_df \n\n    y_train\
          \ = pd.read_csv(ytrain_data.path)\n    print(\"Shape of ytrain_df:\", y_train.shape)\n\
          \    print(\"y_train_df:\", y_train)\n\n    # Debug: Validate splits\n \
          \   print(\"Shapes of X_train and y_train: \")\n    print(\"X_train:\",\
          \ X_train.shape)\n    print(\"y_train:\", y_train.shape) \n    print(\"\
          Missing values in X_train:\", X_train.isnull().sum())\n    print(\"Missing\
          \ values in y_train:\", y_train.isnull().sum()) \n\n    # Ensure no NaN\
          \ values\n    assert not X_train.isnull().values.any(), \"X_train contains\
          \ NaN values.\"\n    assert not y_train.isnull().values.any(), \"y_train\
          \ contains NaN values.\" \n\n    # Train model\n    model = LogisticRegression()\n\
          \    model.fit(X_train, y_train)\n\n    # Save model\n    dump(model, model_output.path)\n\
          \n"
        image: python:3.9
pipelineInfo:
  name: ml-pipeline
root:
  dag:
    tasks:
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        dependentTasks:
        - preprocess-data
        - train-model
        inputs:
          artifacts:
            model:
              taskOutputArtifact:
                outputArtifactKey: model_output
                producerTask: train-model
            test_data:
              taskOutputArtifact:
                outputArtifactKey: output_test
                producerTask: preprocess-data
            ytest_data:
              taskOutputArtifact:
                outputArtifactKey: output_ytest
                producerTask: preprocess-data
        taskInfo:
          name: evaluate-model
      load-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-data
        taskInfo:
          name: load-data
      preprocess-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-data
        dependentTasks:
        - load-data
        inputs:
          artifacts:
            input_csv:
              taskOutputArtifact:
                outputArtifactKey: output_csv
                producerTask: load-data
        taskInfo:
          name: preprocess-data
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - preprocess-data
        inputs:
          artifacts:
            train_data:
              taskOutputArtifact:
                outputArtifactKey: output_train
                producerTask: preprocess-data
            ytrain_data:
              taskOutputArtifact:
                outputArtifactKey: output_ytrain
                producerTask: preprocess-data
        taskInfo:
          name: train-model
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
